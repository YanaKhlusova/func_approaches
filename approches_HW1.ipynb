{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0777c6e",
   "metadata": {},
   "source": [
    "# Corpus driven analysis of Ch. Dickens' novel 'Hard Times'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12abf9",
   "metadata": {},
   "source": [
    "I think this novel is interesting to analyze because of its structure. There are books, named with Bible terms ('Sowing', 'Reaping' and 'Garnering'), and some chapters which headings are also connected to religion. However, nothing is said about it in the book. There are at least 5 different sujet lines under the one cover. So I wanted to do some quantitative research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acc2b73",
   "metadata": {},
   "source": [
    "## Opening and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ed172",
   "metadata": {},
   "source": [
    "I opened texts from my device and preprocessed them with spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e67be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc1b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\yanak\\\\OneDrive\\\\Документы\\\\Dickens_Hard-Times.txt', encoding='utf-8') as txt:\n",
    "        text = txt.read()\n",
    "        text = re.sub('\\n+', ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f4ab58",
   "metadata": {},
   "source": [
    "I noticed some highlited words, ticked with _word_. I decided to count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7ead6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "bold = len(re.findall('_[a-zA-Z]*_', text))\n",
    "print(bold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e68a22c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub('_', '', text) # cleaning from '_'\n",
    "text = re.sub('CHAPTER [A-Z]*', '', text) # cleaning from CHAPTERs - I want to split by books\n",
    "#text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a99a8cc",
   "metadata": {},
   "source": [
    "My decision was to split the novel not in chapters, but in books, because I guess they better demonstrate the differences throughout the whole text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "796c35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_text = re.split('BOOK THE [A-Z]*', text) # splitting into 3 books\n",
    "del splitted_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c505ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # I wrote everything in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5220f832",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOWING  THE ONE THING NEEDFUL ‘NOW, what I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REAPING  EFFECTS IN THE BANK A SUNNY midsumme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GARNERING  ANOTHER THING NEEDFUL LOUISA awoke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0   SOWING  THE ONE THING NEEDFUL ‘NOW, what I wa...\n",
       "1   REAPING  EFFECTS IN THE BANK A SUNNY midsumme...\n",
       "2   GARNERING  ANOTHER THING NEEDFUL LOUISA awoke..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hardtimes_df = pd.DataFrame({\n",
    "    'Text': splitted_text\n",
    "}\n",
    ")\n",
    "hardtimes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5462c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    return nlp(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5ce8011",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardtimes_df['doc'] = hardtimes_df['Text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "87751cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_nosw(doc): # doc w/o stopwords. I'll need to freq. word lists and similarity\n",
    "    return nlp(' '.join([str(token) for token in doc if not token.is_stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fa953711",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardtimes_df['doc_wosw'] = hardtimes_df['doc'].apply(docs_nosw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "70c9febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get lemmas without stopw. I need them to examine lexical richness of the books and some metrics in stylo\n",
    "def get_lemmas_wosw(doc):\n",
    "    return [(token.lemma_) for token in doc if not token.is_punct]\n",
    "hardtimes_df['Lemmas_not_stop'] = hardtimes_df['doc_wosw'].apply(get_lemmas_wosw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a05ef209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the whole list of lemmas I need to examine lexical richness of the books and some metrics in stylo\n",
    "def get_lemmas(doc):\n",
    "    return [(token.lemma_) for token in doc if not token.is_punct]\n",
    "hardtimes_df['Lemmas'] = hardtimes_df['doc'].apply(get_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c37371",
   "metadata": {},
   "source": [
    "To work with Stylo, I saved the lists of lemmas into 3 txt files. The results are in the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "352fbe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\yanak\\\\Documents\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbabb659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I didn't come up with anyth smarter :(\n",
    "with open(path+\"third_lemmas.txt\", \"w+\", encoding=\"utf-8\") as fp:\n",
    "    for i in hardtimes_df['Lemmas'][2]:\n",
    "        fp.write(\"%s\\n\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89accc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac47a64",
   "metadata": {},
   "source": [
    "## NERs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741b02b",
   "metadata": {},
   "source": [
    "Let's observe named entities, SpaCy has various inbuilt labels (ordinals, dates, times...), but I'm interested in GPE's and Person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d8bff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pers(doc):\n",
    "    pers = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            pers.append(ent.text)\n",
    "    return pers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "89430a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardtimes_df['Persons'] = hardtimes_df['doc'].apply(get_pers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0a75ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpes(doc):\n",
    "    gpes = []\n",
    "    for ent in doc.ents: #тут уже речь идет не о токенах, а об именованных сущностях и их списке\n",
    "        if ent.label_ == 'GPE': #если у им. сущности лейбл \"person\"\n",
    "            gpes.append(ent.text) #то мы добавляем эту сущность в список\n",
    "    return gpes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4910aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardtimes_df['GPES'] = hardtimes_df['doc'].apply(get_gpes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "620ecf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def freq_table(lemmas_list): # counts abs freq of lemmas/entities in the texts\n",
    "    counter = Counter(lemmas_list)\n",
    "    freq_dict = dict(counter)\n",
    "    return freq_dict\n",
    "\n",
    "def making_df(freq_dict, col): # makes them a better view\n",
    "    df = pd.DataFrame({\n",
    "        col: freq_dict.keys(),\n",
    "        'Abs_freq': freq_dict.values(),\n",
    "    }\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def sorter(df):\n",
    "    df = df.sort_values(by=['Abs_freq'], ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e284c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardtimes_df['Pers_freq'] = hardtimes_df['Persons'].apply(freq_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a886114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardtimes_df['GPEs_freq'] = hardtimes_df['GPES'].apply(freq_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79a0b1",
   "metadata": {},
   "source": [
    "There will be the most frequent entities with label 'Person'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5efedc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Abs_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gradgrind</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bounderby</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>louisa</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>sparsit</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>tom</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>stephen</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>rachael</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jupe</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>thomas</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>thquire</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thomas gradgrind</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>m’choakumchild</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>tom gradgrind</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>childers</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>sleary</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Entity  Abs_freq\n",
       "7          gradgrind       143\n",
       "25         bounderby       115\n",
       "22            louisa       108\n",
       "61           sparsit        54\n",
       "34               tom        50\n",
       "72           stephen        39\n",
       "73           rachael        34\n",
       "8               jupe        29\n",
       "24            thomas        23\n",
       "48           thquire        21\n",
       "1   thomas gradgrind        14\n",
       "13    m’choakumchild        14\n",
       "65     tom gradgrind        11\n",
       "40          childers        11\n",
       "47            sleary         9"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persons_book1 = making_ent_df(hardtimes_df['Pers_freq'][0])\n",
    "persons_book1 = sorter(persons_book1)\n",
    "persons_book1[:15] # there are more of them, but the vast majority is hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc005ff",
   "metadata": {},
   "source": [
    "As you may see, the most freq. entity is Gradgrind. However, there is not one person: it could be Thomas Gradgrind as well as his father. Sissy (Cecilia) Jupe has only 8th rank despite the fact that first book is devoted to her story. I think this is because spaCy didn't recognize her name. Thquire - Esquire; one of characters has troubles in pronouncing this word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d12bd038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Abs_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sparsit</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bounderby</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tom</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>stephen</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>harthouse</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>louisa</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>james harthouse</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>rachael</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gradgrind</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tom gradgrind</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tom.</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>pegler</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>coom</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jem</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>stephen blackpool</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Entity  Abs_freq\n",
       "1             sparsit       193\n",
       "0           bounderby       142\n",
       "17                tom       101\n",
       "32            stephen        65\n",
       "10          harthouse        65\n",
       "14             louisa        64\n",
       "6     james harthouse        37\n",
       "42            rachael        31\n",
       "4           gradgrind        15\n",
       "11      tom gradgrind         9\n",
       "18             tom.           9\n",
       "65             pegler         7\n",
       "31               coom         7\n",
       "7                 jem         6\n",
       "23  stephen blackpool         6"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persons_book2 = making_ent_df(hardtimes_df['Pers_freq'][1])\n",
    "persons_book2 = sorter(persons_book2)\n",
    "persons_book2[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e3bb6",
   "metadata": {},
   "source": [
    "Sissy disappears in the 2nd book. We may notice it even using AntConc (see distribution of her name throughout the whole novel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1259becb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Abs_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>louisa</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bounderby</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rachael</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gradgrind</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sparsit</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>sleary</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>thquire</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>stephen blackpool</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tom</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>stephen</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tom gradgrind</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>pegler</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>harthouse</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>james harthouse</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>coketown</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Entity  Abs_freq\n",
       "0              louisa        68\n",
       "6           bounderby        68\n",
       "22            rachael        67\n",
       "2           gradgrind        61\n",
       "7             sparsit        43\n",
       "45             sleary        25\n",
       "62            thquire        22\n",
       "19  stephen blackpool        21\n",
       "8                 tom        21\n",
       "26            stephen        18\n",
       "12      tom gradgrind        17\n",
       "35             pegler        16\n",
       "10          harthouse        12\n",
       "4     james harthouse        10\n",
       "15           coketown         4"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persons_book3 = making_ent_df(hardtimes_df['Pers_freq'][2])\n",
    "persons_book3 = sorter(persons_book3)\n",
    "persons_book3[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962caa17",
   "metadata": {},
   "source": [
    "Accidentally, Coketown is recognized as person, not gpe. So, based on this three lists of persons, the most mentioned characters are: Gradgrinds, Bounderby, Rachel, Stephen and Louisa. They have 2-3 own stories (sujet lines), but there are more of them with other characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b16d1e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Abs_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>louisa</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>england</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>london</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>thort</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>st</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ark</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wapping</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bitterth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>billth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thtick</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>wurth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>calais</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>wales</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>india</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>china</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Entity  Abs_freq\n",
       "3     louisa         4\n",
       "0    england         3\n",
       "2     london         2\n",
       "7      thort         2\n",
       "1         st         1\n",
       "4        ark         1\n",
       "5    wapping         1\n",
       "6   bitterth         1\n",
       "8     billth         1\n",
       "9     thtick         1\n",
       "10     wurth         1\n",
       "11    calais         1\n",
       "12     wales         1\n",
       "13     india         1\n",
       "14     china         1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpes_book1 = making_ent_df(hardtimes_df['GPEs_freq'][0])\n",
    "gpes_book1 = sorter(gpes_book1)\n",
    "gpes_book1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88231e2",
   "metadata": {},
   "source": [
    "As was already mentioned, the story takes place in Coketown - imagined city in the UK of Victorian era. So,  gpes here are somehow connected to England (England, London), Wales, India. Surprisingly, there is China, but I don't remember, why. Accidentally Louisa is recognized as GPE. Some strange words are also marked with this label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "296b3993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Abs_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>london</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>yorkshire</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jerusalem</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>india</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yo’d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>seabeach</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>romulus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rome</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>yorick</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sir!—in</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>drivelling</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>china</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>discordantly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>yoong</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>castlereagh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ireland</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>britain</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>louisa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Entity  Abs_freq\n",
       "1         london         6\n",
       "19     yorkshire         2\n",
       "2      jerusalem         2\n",
       "18         india         2\n",
       "6           yo’d         2\n",
       "12      seabeach         1\n",
       "17       romulus         1\n",
       "16          rome         1\n",
       "15        yorick         1\n",
       "14       sir!—in         1\n",
       "13    drivelling         1\n",
       "0          china         1\n",
       "11          weel         1\n",
       "9           stan         1\n",
       "8   discordantly         1\n",
       "7          yoong         1\n",
       "5    castlereagh         1\n",
       "4        ireland         1\n",
       "3        britain         1\n",
       "10        louisa         1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpes_book2 = making_ent_df(hardtimes_df['GPEs_freq'][1])\n",
    "gpes_book2 = sorter(gpes_book2)\n",
    "gpes_book2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693376cc",
   "metadata": {},
   "source": [
    "In the 2nd book there are more names of cities/countries: London, Yorkshire, Rome, Ireland, Britain, India, China, Jerusalem. This not means that the action moves somewhere from Coketown. I think this GPEs come from dialogues or character's stories. Louisa is GPE again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "718b0fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Abs_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>louisa</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>liverpool</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lancashire</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>st</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dree</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>redeemer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>japan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>josephine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>harneth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>animalth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>o yeth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>thingth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entity  Abs_freq\n",
       "2       louisa         8\n",
       "5    liverpool         4\n",
       "0   lancashire         1\n",
       "1           st         1\n",
       "3         dree         1\n",
       "4     redeemer         1\n",
       "6        japan         1\n",
       "7    josephine         1\n",
       "8      harneth         1\n",
       "9     animalth         1\n",
       "10      o yeth         1\n",
       "11     thingth         1"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpes_book3 = making_ent_df(hardtimes_df['GPEs_freq'][2])\n",
    "gpes_book3 = sorter(gpes_book3)\n",
    "gpes_book3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf2542",
   "metadata": {},
   "source": [
    "The 3rd book has less chapters and less entities also. Here we have only Liverpool and Japan as real locations. Others are personal names or mispronounced words.\n",
    "\n",
    "So, the main geographical location in this novel is England and its surroundings. There are also mentions of east countries (India, Japan, China)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c79d6",
   "metadata": {},
   "source": [
    "## Frequency word lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb3c19b",
   "metadata": {},
   "source": [
    "I compiled freq. word lists without stopwords, but to be honest, I don't agree that they have much information under these conditions. I displayed 50 the most frequent lemmas and am ready to argue that they are typical to every english fiction book. Hence they don't have much meaning and can be seen as stopwords for this type of texts, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c2102cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardtimes_df['Lemmas_freq'] = hardtimes_df['Lemmas_not_stop'].apply(freq_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "630d6a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Abs_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>say</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>mr</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>gradgrind</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>bounderby</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>know</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>father</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>louisa</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>look</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>come</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>mrs</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>go</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>little</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sir</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>hand</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>good</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fact</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>time</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>like</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>eye</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>tom</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>face</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>old</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>sissy</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>think</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>girl</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>hear</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>head</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>see</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>woman</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>stephen</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>sparsit</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>return</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>man</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>ask</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>young</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>take</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>room</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>tell</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>great</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>away</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>work</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>long</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>people</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>way</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>night</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>year</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>get</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>thomas</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>dear</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Lemma  Abs_freq\n",
       "188         say       299\n",
       "151          mr       262\n",
       "112   gradgrind       197\n",
       "954   bounderby       181\n",
       "191        know       148\n",
       "199      father       127\n",
       "882      louisa       118\n",
       "294        look       118\n",
       "133        come       111\n",
       "965         mrs        95\n",
       "415          go        92\n",
       "97       little        90\n",
       "21          sir        89\n",
       "211        hand        87\n",
       "489        good        85\n",
       "5          fact        78\n",
       "314        time        78\n",
       "73         like        73\n",
       "45          eye        67\n",
       "631         tom        67\n",
       "250        face        66\n",
       "606         old        62\n",
       "192       sissy        62\n",
       "562       think        62\n",
       "8          girl        60\n",
       "337        hear        60\n",
       "65         head        59\n",
       "388         see        58\n",
       "429       woman        58\n",
       "2730    stephen        57\n",
       "2163    sparsit        56\n",
       "201      return        54\n",
       "113         man        54\n",
       "381         ask        52\n",
       "183       young        52\n",
       "526        take        51\n",
       "28         room        50\n",
       "131        tell        49\n",
       "226       great        48\n",
       "186        away        46\n",
       "518        work        46\n",
       "448        long        45\n",
       "330      people        44\n",
       "329         way        44\n",
       "1036      night        44\n",
       "577        year        43\n",
       "1090        get        43\n",
       "111      thomas        43\n",
       "1180       dear        42"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_book1 = making_df(hardtimes_df['Lemmas_freq'][0], 'Lemma')\n",
    "lemmas_book1 = sorter(lemmas_book1)\n",
    "lemmas_book1[1:50] # the 1st row are spaces which were added instead of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3391b41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Abs_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>say</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>mrs</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>bounderby</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mr</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>sparsit</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>know</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>tom</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>harthouse</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>man</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>look</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>sir</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>ma’am</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>good</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>come</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>go</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>stephen</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>think</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>hand</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>time</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>little</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>louisa</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>see</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>return</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>face</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>tell</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>like</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>hear</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>day</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>take</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>head</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>way</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>old</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>eye</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>fellow</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>’em</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>o</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>turn</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>great</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>well</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>gradgrind</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>bitzer</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>ask</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>coketown</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>night</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>light</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>life</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>house</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>mind</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>find</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Lemma  Abs_freq\n",
       "449         say       276\n",
       "269         mrs       232\n",
       "89    bounderby       214\n",
       "88           mr       214\n",
       "270     sparsit       196\n",
       "19         know       174\n",
       "1216        tom       120\n",
       "1060  harthouse       110\n",
       "520         man       104\n",
       "76         look       100\n",
       "814         sir        95\n",
       "483       ma’am        88\n",
       "128        good        88\n",
       "111        come        86\n",
       "192          go        84\n",
       "1853    stephen        81\n",
       "820       think        79\n",
       "167        hand        77\n",
       "454        time        76\n",
       "228      little        72\n",
       "1328     louisa        71\n",
       "9           see        69\n",
       "484      return        66\n",
       "518        face        64\n",
       "966        tell        63\n",
       "176        like        62\n",
       "492        hear        61\n",
       "6           day        58\n",
       "97         take        53\n",
       "191        head        53\n",
       "29          way        52\n",
       "500         old        52\n",
       "263         eye        52\n",
       "885      fellow        49\n",
       "727         ’em        46\n",
       "1056          o        46\n",
       "816        turn        46\n",
       "534       great        45\n",
       "651        well        45\n",
       "947   gradgrind        45\n",
       "482      bitzer        44\n",
       "498         ask        43\n",
       "8      coketown        43\n",
       "213       night        43\n",
       "46        light        42\n",
       "122        life        42\n",
       "308       house        41\n",
       "590        mind        40\n",
       "1067       find        40"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_book2 = making_df(hardtimes_df['Lemmas_freq'][1], 'Lemma')\n",
    "lemmas_book2 = sorter(lemmas_book2)\n",
    "lemmas_book2[1:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "af0535e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Abs_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>say</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>mr</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>bounderby</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>know</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>come</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>go</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>man</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>louisa</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>gradgrind</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>sissy</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>rachael</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>night</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>mrs</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>look</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>hand</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>time</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>think</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>father</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>way</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>sir</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>dear</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320</th>\n",
       "      <td>stephen</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>young</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>tell</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>sparsit</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>away</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>bring</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>day</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>return</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>find</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>leave</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>sleary</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>tom</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>old</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>like</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eye</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>face</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>little</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>well</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>good</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>lady</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>take</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>word</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>see</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>great</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>stand</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thing</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>head</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>town</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Lemma  Abs_freq\n",
       "68          say       180\n",
       "242          mr       179\n",
       "521   bounderby       145\n",
       "206        know       120\n",
       "59         come       118\n",
       "62           go        87\n",
       "238         man        84\n",
       "4        louisa        82\n",
       "243   gradgrind        81\n",
       "55        sissy        79\n",
       "1591    rachael        75\n",
       "54        night        70\n",
       "522         mrs        64\n",
       "46         look        64\n",
       "51         hand        60\n",
       "41         time        60\n",
       "78        think        60\n",
       "67       father        56\n",
       "146         way        51\n",
       "676         sir        51\n",
       "122        dear        49\n",
       "1320    stephen        44\n",
       "74        young        44\n",
       "69         tell        44\n",
       "531     sparsit        44\n",
       "342        away        43\n",
       "53        bring        43\n",
       "15          day        42\n",
       "399      return        42\n",
       "57         find        41\n",
       "336       leave        41\n",
       "2389     sleary        41\n",
       "536         tom        40\n",
       "10          old        39\n",
       "355        like        39\n",
       "7           eye        38\n",
       "72         face        38\n",
       "37       little        38\n",
       "162        well        37\n",
       "296        good        37\n",
       "685        lady        36\n",
       "64         take        36\n",
       "121        word        36\n",
       "229         see        36\n",
       "192       great        36\n",
       "137       stand        35\n",
       "2         thing        33\n",
       "26         head        33\n",
       "523        town        33"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_book3 = making_df(hardtimes_df['Lemmas_freq'][2], 'Lemma')\n",
    "lemmas_book3 = sorter(lemmas_book3)\n",
    "lemmas_book3[1:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdcd8b5",
   "metadata": {},
   "source": [
    "The only difference here are proper names and adjectives 'old' and 'young'. And I'm not really sure that it have much meaning. We can just conclude smth about charachters appeared in different parts of the novel and nothing more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7edc2c3",
   "metadata": {},
   "source": [
    "## Similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d54552f",
   "metadata": {},
   "source": [
    "Let's see how similar the books to each other using spaCy function similarity. I started with docs without stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d6b54a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Book 1 and Book 2 =  0.9991396969291683\n"
     ]
    }
   ],
   "source": [
    "print('Similarity between Book 1 and Book 2 = ', hardtimes_df['doc_wosw'][0].similarity(hardtimes_df['doc_wosw'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5256025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Book 2 and Book 3 =  0.9979373029412386\n"
     ]
    }
   ],
   "source": [
    "print('Similarity between Book 2 and Book 3 = ', hardtimes_df['doc_wosw'][1].similarity(hardtimes_df['doc_wosw'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1645208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Book 1 and Book 3 =  0.9987592511843397\n"
     ]
    }
   ],
   "source": [
    "print('Similarity between Book 1 and Book 3 = ', hardtimes_df['doc_wosw'][0].similarity(hardtimes_df['doc_wosw'][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d4f87",
   "metadata": {},
   "source": [
    "All 3 books are almost similar to each other. I guess this is because they are the parts of one novel and they all have the same authorship (what means that there are no or minor lexical, thematical and stylistical differences)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beaa60f",
   "metadata": {},
   "source": [
    "Let's try docs WITH stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad0be128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Book 1 and Book 2 =  0.9995799683823092\n"
     ]
    }
   ],
   "source": [
    "print('Similarity between Book 1 and Book 2 = ', hardtimes_df['doc'][0].similarity(hardtimes_df['doc'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a9a1092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Book 2 and Book 3 =  0.9989907808111365\n"
     ]
    }
   ],
   "source": [
    "print('Similarity between Book 2 and Book 3 = ', hardtimes_df['doc'][1].similarity(hardtimes_df['doc'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22325d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Book 1 and Book 3 =  0.9984452025083468\n"
     ]
    }
   ],
   "source": [
    "print('Similarity between Book 1 and Book 3 = ', hardtimes_df['doc'][0].similarity(hardtimes_df['doc'][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45c328",
   "metadata": {},
   "source": [
    "Well, I can see no huge difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095c80b",
   "metadata": {},
   "source": [
    "## Lexical Diversity (Richness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b3db2",
   "metadata": {},
   "source": [
    "The following measures show to what extent is vocabulary of different parts of the novel diversive. I guess you know about TTR (type-token ratio) and problems with it, so I used more insensitive to text length measures. In fact, this code and the whole idea are stolen from my bachelor thesis, so I was able to bring references to compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1db8d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lexicalrichness\n",
    "from lexicalrichness import LexicalRichness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4e5980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_rich(lst_lemmas):\n",
    "    return LexicalRichness(lst_lemmas, preprocessor=None, tokenizer=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec5d3b1",
   "metadata": {},
   "source": [
    "Excuse me I really have troubles with cycles, I'll work on it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e92cb3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex1 = lex_rich(hardtimes_df['Lemmas'][0])\n",
    "lex2 = lex_rich(hardtimes_df['Lemmas'][1])\n",
    "lex3 = lex_rich(hardtimes_df['Lemmas'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041f6ec",
   "metadata": {},
   "source": [
    "NB! The bigger number is, the more lexically rich is the book. Except Maas metric: it uses logarythm, so the smaller the number is, then more diversive is vocabulary. If you want to read full description of metrics, please, google the module used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb5a7836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book1\n",
      "MSTTR =  0.666 \n",
      "MATTR =  0.667 \n",
      "Maas =  0.0203 \n",
      "MTLD =  62.4766 \n",
      "HDD =  0.8473\n"
     ]
    }
   ],
   "source": [
    "per_msttr = round(lex1.msttr(), 4)\n",
    "per_mattr = round(lex1.mattr(), 4)\n",
    "per_maas = round(lex1.Maas, 4)\n",
    "per_mtld = round(lex1.mtld(), 4)\n",
    "per_hdd = round(lex1.hdd(), 4)\n",
    "print('Book1\\nMSTTR =',per_msttr,'\\nMATTR =',per_mattr,'\\nMaas =',per_maas,'\\nMTLD =',per_mtld,'\\nHDD =',per_hdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4c45311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book2\n",
      "MSTTR = 0.6759 \n",
      "MATTR = 0.6763 \n",
      "Maas = 0.0203 \n",
      "MTLD = 66.515 \n",
      "HDD = 0.8494\n"
     ]
    }
   ],
   "source": [
    "per_msttr = round(lex2.msttr(), 4)\n",
    "per_mattr = round(lex2.mattr(), 4)\n",
    "per_maas = round(lex2.Maas, 4)\n",
    "per_mtld = round(lex2.mtld(), 4)\n",
    "per_hdd = round(lex2.hdd(), 4)\n",
    "print('Book2\\nMSTTR =',per_msttr,'\\nMATTR =',per_mattr,'\\nMaas =',per_maas,'\\nMTLD =',per_mtld,'\\nHDD =',per_hdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a6e1a915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book3\n",
      "MSTTR = 0.6737 \n",
      "MATTR = 0.672 \n",
      "Maas = 0.0203 \n",
      "MTLD = 65.3065 \n",
      "HDD = 0.8444\n"
     ]
    }
   ],
   "source": [
    "per_msttr = round(lex3.msttr(), 4)\n",
    "per_mattr = round(lex3.mattr(), 4)\n",
    "per_maas = round(lex3.Maas, 4)\n",
    "per_mtld = round(lex3.mtld(), 4)\n",
    "per_hdd = round(lex3.hdd(), 4)\n",
    "print('Book3\\nMSTTR =',per_msttr,'\\nMATTR =',per_mattr,'\\nMaas =',per_maas,'\\nMTLD =',per_mtld,'\\nHDD =',per_hdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee001642",
   "metadata": {},
   "source": [
    "Are these numbers big or not? Well, to compare, russian short stories from 20th century have the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "325fe382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decade</th>\n",
       "      <th>MSTTR</th>\n",
       "      <th>MATTR</th>\n",
       "      <th>Maas</th>\n",
       "      <th>MTLD</th>\n",
       "      <th>HD-D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00th</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.017</td>\n",
       "      <td>139.516</td>\n",
       "      <td>0.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10th</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.017</td>\n",
       "      <td>147.776</td>\n",
       "      <td>0.918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20th</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.016</td>\n",
       "      <td>158.578</td>\n",
       "      <td>0.929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20th</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.016</td>\n",
       "      <td>161.277</td>\n",
       "      <td>0.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40th</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.017</td>\n",
       "      <td>154.924</td>\n",
       "      <td>0.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50th</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.017</td>\n",
       "      <td>166.633</td>\n",
       "      <td>0.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60th</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.016</td>\n",
       "      <td>158.059</td>\n",
       "      <td>0.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>70th</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.016</td>\n",
       "      <td>144.925</td>\n",
       "      <td>0.916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>80th</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.016</td>\n",
       "      <td>147.921</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>90th-2000</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.015</td>\n",
       "      <td>172.487</td>\n",
       "      <td>0.925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Decade  MSTTR  MATTR   Maas     MTLD   HD-D\n",
       "0       00th  0.775  0.774  0.017  139.516  0.913\n",
       "1       10th  0.777  0.777  0.017  147.776  0.918\n",
       "2       20th  0.787  0.788  0.016  158.578  0.929\n",
       "3       20th  0.788  0.788  0.016  161.277  0.927\n",
       "4       40th  0.784  0.784  0.017  154.924  0.922\n",
       "5       50th  0.790  0.790  0.017  166.633  0.922\n",
       "6       60th  0.787  0.787  0.016  158.059  0.923\n",
       "7       70th  0.776  0.775  0.016  144.925  0.916\n",
       "8       80th  0.780  0.779  0.016  147.921  0.917\n",
       "9  90th-2000  0.795  0.795  0.015  172.487  0.925"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rss_lexrich_df = pd.read_csv('C:/Users/yanak/Documents/lex_rich_decades_new.txt', sep='\\t')\n",
    "rss_lexrich_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866dad9",
   "metadata": {},
   "source": [
    "There were analyzed 1000 stories from 807 different authors. The stories were divided into subcorpora of 100 texts by every decade of the century. About 80-100 authors by each decade. So, vocabulary there is more rich than in books from 'Hard Times'. I guess that is so because of differences between languages and authorical styles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4326aef0",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b490a1",
   "metadata": {},
   "source": [
    "So, there are some differencies between books in characters and locations. However, basing on quantitative research, I can say that the main focus is on Gradgrinds family, mr. Bounderby, Rachel and Stephen Blackpool. The action takes place in England, though there are minor mentions of east countries. The lexicon is rather and at quite similar level diversive in all 3 books."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
